{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、参数估计\n",
    "\n",
    "参数是总体分布中的参数,反映的是总体某方面特征的量。例如：合格率，均值，方差，中位数等。参数估计问题是利用从总体抽样得到的信息来估计总体的某些参数或者参数的某些函数。\n",
    "问题的一般提法\n",
    "\n",
    "##### 问题的一般提法\n",
    "\n",
    "设有一个统计总体，总体的分布函数为$F(x,\\theta)$，其中$\\theta$为未知参数。现从该总体取样本$X_1,X_2,...,X_n$，要依据样本对参数$\\theta$作出估计，或估计$\\theta$的某个已知函数$g(\\theta)$。这类问题称为参数估计。\n",
    "\n",
    "#####参数估计好的评价标准\n",
    "\n",
    "由于存在不同的方法对总体中的未知参数进行估计，利用这些不同的方法得到的估计值也不同。因此就涉及到如何评价不同估计量的好坏的问题。\n",
    "\n",
    "常用的评价准则有以下四条：\n",
    "\n",
    "- 无偏性准则\n",
    "- 有效性准则\n",
    "- 均方误差准则\n",
    "- 相合性准则\n",
    "\n",
    "更多信息可以查看:https://www.cnblogs.com/Belter/p/8337992.html\n",
    "\n",
    "#### 二、最小二乘法\n",
    "\n",
    "1. 目标函数：训练误差方差$\\varepsilon=\\sum\\limits_{i=1}^m(y^{(i)}-h_\\theta{(x_i)})^2$  使$\\varepsilon$最小$\\implies$ $h_\\theta{(x)} \\approxeq y_{真值} $\n",
    "\n",
    "2. 目标函数转换为矩阵表示$J(\\vec{\\theta}) =\\varepsilon=\\sum\\limits_{i=1}^m(y^{(i)}-h_\\vec{\\theta}{(x_i)})^2 = (X\\theta-y)^T(X\\theta-y)$\n",
    "\n",
    "   \n",
    "\n",
    "3. $\\varepsilon$是一个二次凹函数，根据导数运算法则和矩阵转置法则1. $(uv)'=u'v+uv'$，2. $A^TB=B^TA$，\n",
    "\n",
    "   对其求导：$\\frac{\\partial \\varepsilon}{\\partial\\theta} = X^T(X\\theta-y) + (X\\theta-y)^T X =  X^T(X\\theta-y) + X^T(X\\theta-y) =2X^TX\\theta-2X^Ty$ \n",
    "\n",
    "   \n",
    "\n",
    "4. 令$2X^TX\\theta-2X^Ty=0$ 得：$\\theta = (X^TX)^{-1}X^Ty$   \n",
    "\n",
    "5. 当矩阵$X$可逆时，可以用$\\theta = (X^TX)^-1X^Ty$   求解$\\theta$,当矩阵不可逆时，可以通过梯度下降求解：\n",
    "\n",
    "   $\\theta = \\theta-\\alpha\\frac{\\partial \\varepsilon}{\\partial\\theta} = \\theta - (2X^TX\\theta-2X^Ty)$，$\\alpha$是下降的步长, 迭代次数t\n",
    "\n",
    "常用的梯度下降算法有SGD,BGD,mBGD,实际中以mBGD使用最多。\n",
    "\n",
    "6. 线性回归防止过拟合，常常采取加入正则化项:\n",
    "\n",
    "   $J(\\vec{\\theta}) = \\sum\\limits_{i=1}^m(y^{(i)}-h_\\vec{\\theta}{(x_i)})^2 +\\lambda\\sum\\limits_{j=1}^n\\theta_j^2 $\n",
    "\n",
    "   * 当矩阵可逆时 $\\theta = (X^TX + \\lambda I)^{-1}X^Ty $\n",
    "   * 当矩阵不满秩时，$\\theta_j :=\\theta_j (1-\\frac{\\alpha}{m}) - \\alpha\\frac{1}{m}\\sum(h_\\theta{(x_j)-y})x_j$\n",
    "\n",
    "所以最小二乘法可以根据样本来拟合曲线，从而对其他样本的值进行预测。\n",
    "\n",
    "#### 三、极大似然法(MLE)\n",
    "\n",
    "$ p(\\theta|X) = \\frac{p(X|\\theta)*p(\\theta)}{p(X)}$\n",
    "\n",
    "$ posterior = \\frac{likelihood*prior}{evidence}$\n",
    "\n",
    "最大似然估计就是要用似然函数取到最大值时的参数值作为估计值,由于$X$的每个样本都是独立同分布，似然函数可以写作：\n",
    "\n",
    "$L(\\theta|X) = p(X|\\theta) = \\prod\\limits_{x\\in{X}}p(X=x|\\theta)$ \n",
    "\n",
    "由于有脸乘运算，通常对似然函数取对数计算简便,即对数似然函数，最大似然函数可以写成:\n",
    "\n",
    "$argmax_\\theta L(\\theta|X) = argmax_\\theta\\sum\\limits_{x\\in{X}}logp(x|\\theta) $,这是一个关于$\\theta$的函数，求解这个优化问题通常对求导$\\theta$，得到导数为0的极值点。该函数取得最大值是对应的$\\theta$的取值就是我们估计的模型参数。\n",
    "\n",
    "**极大似然法等价于以KL散度为损失函数的最小化**\n",
    "\n",
    "##### 举例\n",
    "\n",
    "以扔硬币的伯努利实验为例子，N次实验的结果服从二项分布，参数为P，即每次实验事件发生的概率，不妨设为是得到正面的概率。为了估计P，采用最大似然估计，似然函数可以写作\n",
    "\n",
    "$L=\\log\\prod\\limits_i^Np(C=c_i|p) = \\sum\\limits_{i=1}^N\\log p(C=c_i|p)$\n",
    "\n",
    "$=n^{(1)}\\log p + n^{(0)}\\log (1-p)$\n",
    "\n",
    "令$n^{(1)}\\log p + n^{(0)}\\log (1-p)=0$ 得 $p=\\frac{n^{(1)}}{n^{(1)} + n^{(0)}}$\n",
    "\n",
    "#### 四、最大后验估计法(MAP)\n",
    "\n",
    "最大后验估计与最大似然估计相似，不同点在于估计$\\theta$的函数中允许加入一个先验$p(\\theta)$，也就是说此时不是要求似然函数最大，而是要求由贝叶斯公式计算出的整个后验概率最大，即\n",
    "\n",
    "$\\hat{\\theta}_{MAP} = argmax_\\theta\\frac{p(X|\\theta)p(\\theta)}{p(X)}$\n",
    "\n",
    "$= argmax_\\theta p(X|\\theta)p(\\theta)$\n",
    "\n",
    "$= argmax_\\theta \\{L(\\theta|X) + \\log p(\\theta)\\}$\n",
    "\n",
    "$= argmax_\\theta \\{\\sum\\limits_{x\\in{X}} + \\log p(\\theta)\\}$\n",
    "\n",
    "注意这里P(X)与参数 $ \\theta$ 无关，因此等价于要使分子最大。与最大似然估计相比，现在需要多加上一个先验分布概率的对数。在实际应用中，这个先验可以用来描述人们已经知道或者接受的普遍规律。例如在扔硬币的试验中，每次抛出正面发生的概率应该服从一个概率分布，这个概率在0.5处取得最大值，这个分布就是先验分布。\n",
    "\n",
    "\n",
    "\n",
    "- 频率学派 - Frequentist - Maximum Likelihood Estimation (MLE，最大似然估计)\n",
    "- 贝叶斯学派 - Bayesian - Maximum A Posteriori (MAP，最大后验估计)\n",
    "\n",
    "在对事物建模时，用 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+) 表示模型的参数，**请注意，解决问题的本质就是求 $ \\theta$  。**那么：\n",
    "\n",
    "**(1) 频率学派：**存在唯一真值 $ \\theta$  。\n",
    "\n",
    "**(2) 贝叶斯学派：** $ \\theta$是一个随机变量，符合一定的概率分布。在贝叶斯学派里有两大输入和一大输出，输入是先验 (prior)和似然 (likelihood)，输出是后验 (posterior)。*先验*，即$P(\\theta)$ ，指的是在没有观测到任何数据时对 $ \\theta$ 的预先判断，例如给我一个硬币，一种可行的先验是认为这个硬币有很大的概率是均匀的，有较小的概率是是不均匀的.\n",
    "\n",
    "#### 五、贝叶斯估计(BOA)\n",
    "\n",
    "贝叶斯估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定概率分布。回顾一下贝叶斯公式\n",
    "\n",
    "$ p(\\theta|X) = \\frac{p(X|\\theta)*p(\\theta)}{p(X)}$\n",
    "\n",
    "现在不是要求后验概率最大，这样就需要求$p(X)$,即观察到的evidence的概率，由全概率公式展开可得\n",
    "\n",
    "$p(X) = \\int_{\\theta\\in{\\Theta}}p(X|\\theta)p(\\theta)d\\theta$\n",
    "\n",
    "当新的数据被观察到时，后验概率可以自动随之调整。但是通常这个全概率的求法是贝叶斯估计比较有技巧性的地方。\n",
    "\n",
    "#### 六、对比总结\n",
    "\n",
    "1. 极大似然估计、最大后验估计和贝叶斯估计都是**参数估计方法**。\n",
    "2. **极大似然估计和最大后验估计都是点估计**，即把参数看成未知常数，通过最大化似然和后验概率实现。\n",
    "3. **贝叶斯估计把参数看成一个随机变量，属于分布估计**，然后求该随机变量在数据集D下的条件期望。\n",
    "4. **当先验为均匀分布时**，极大似然估计和最大后验估计是等价的。即估计参数的先验概率为 1 ；\n",
    "5. **当先验和似然都是高斯分布时**，最大后验估计和贝叶斯估计是等价的。\n",
    "6. 通常情况下，贝叶斯估计的积分很难计算，但可以采取一些近似方法，如拉普拉斯和变分近似以及马尔科夫链蒙特卡洛抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
